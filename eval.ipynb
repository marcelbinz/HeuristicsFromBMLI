{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import kl_divergence, Bernoulli\n",
    "from tqdm import tqdm\n",
    "from environments import PairedComparison\n",
    "from model import GRU\n",
    "from baselines import gini, Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalFirstCue, VariationalBestCue, VariationalFirstDiscriminatingCue, StrategySelection, FeedForward\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed = torch.load('data/humans_ranking.pth')\n",
    "        output_file = 'data/exp1.csv'\n",
    "\n",
    "    elif condition == 2:\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed = torch.load('data/humans_direction.pth')\n",
    "        output_file = 'data/exp2.csv'\n",
    "\n",
    "    elif condition == 3:\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed = torch.load('data/humans_none.pth')\n",
    "        output_file = 'data/exp3.csv'\n",
    "        \n",
    "    elif condition == 4:\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, weights, direction1, direction2, ranking = torch.load('data/humans_2features.pth')\n",
    "        output_file = 'data/exp4.csv'\n",
    "\n",
    "    features = inputs_a - inputs_b\n",
    "    num_participants = features.shape[0]\n",
    "    num_tasks = features.shape[2]\n",
    "    num_steps = features.shape[1]\n",
    "\n",
    "    print(time_elapsed.shape)\n",
    "    print(num_participants, num_tasks, num_steps)\n",
    "\n",
    "    records = np.empty((num_participants * num_tasks * num_steps, 10))\n",
    "    c = 0\n",
    "    for participant in range(num_participants):\n",
    "        for task in range(num_tasks):\n",
    "            for step in range(num_steps):\n",
    "                records[c, 0] = participant\n",
    "                records[c, 1] = task\n",
    "                records[c, 2] = step\n",
    "                records[c, 3] = features[participant, step, task, 0]\n",
    "                records[c, 4] = features[participant, step, task, 1]\n",
    "                if condition < 4:\n",
    "                    records[c, 5] = features[participant, step, task, 2]\n",
    "                    records[c, 6] = features[participant, step, task, 3]                   \n",
    "                records[c, 7] = predictions[participant, step, task, 0]\n",
    "                records[c, 8] = targets[participant, step, task, 0]\n",
    "                records[c, 9] = time_elapsed[participant, step, task, 0]\n",
    "                c += 1\n",
    "\n",
    "\n",
    "    np.savetxt(output_file,\n",
    "        records,\n",
    "        fmt=\",\".join([\"%d\"] + [\"%d\"] + [\"%d\"] + [\"%.2f\"] + [\"%.2f\"] + [\"%.2f\"] + [\"%.2f\"] + [\"%d\"] + [\"%d\"] + [\"%.2f\"]),\n",
    "        header=\"participant,task,step,x0,x1,x2,x3,choice,target,time\",\n",
    "        comments='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "num_episodes = 30\n",
    "sequence_length = 10\n",
    "\n",
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        save_path = 'data/baselines_ranking.pth'\n",
    "        models = [VariationalProbitRegression, VariationalEqualWeighting, VariationalFirstCue]\n",
    "        ranking = True\n",
    "        direction = False\n",
    "        num_features = 4\n",
    "    elif condition == 2:\n",
    "        save_path = 'data/baselines_direction.pth'\n",
    "        models = [VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "        ranking = False\n",
    "        direction = True\n",
    "        num_features = 4\n",
    "    elif condition == 3:\n",
    "        save_path = 'data/baselines_none.pth'\n",
    "        models = [VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 4\n",
    "    elif condition == 4:\n",
    "        save_path = 'data/baselines_none_2cues.pth'\n",
    "        models = [VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 2\n",
    "\n",
    "    data_loader = PairedComparison(num_features, ranking=ranking, direction=direction, dichotomized=False)\n",
    "\n",
    "    map_performance = torch.zeros(len(models), num_experiments, num_episodes, sequence_length)\n",
    "    avg_performance = torch.zeros(len(models), num_experiments, num_episodes, sequence_length)\n",
    "    gini_coefficients = torch.zeros(1, num_experiments, num_episodes, sequence_length)\n",
    "\n",
    "    for j, model_class in enumerate(models):\n",
    "        for k in tqdm(range(num_experiments)):\n",
    "            for i in range(num_episodes):\n",
    "                inputs, targets, _, _ = data_loader.get_batch(1, sequence_length)\n",
    "                model = model_class(data_loader.num_inputs, noise=data_loader.sigma)\n",
    "                predictive_distribution = model.forward(inputs, targets)\n",
    "\n",
    "                prediction = (predictive_distribution.probs > 0.5).float()\n",
    "                map_performance[j, k, i] = (prediction == targets).squeeze()\n",
    "                avg_performance[j, k, i] = ((1 - targets) * (1 - predictive_distribution.probs) + targets * predictive_distribution.probs).squeeze()\n",
    "\n",
    "                if j == 0:\n",
    "                    for t in range(sequence_length):\n",
    "                        gini_coefficients[j, k, i, t] = gini(torch.abs(model.weights[t].t()).squeeze().detach().cpu().numpy())\n",
    "\n",
    "    print(map_performance.mean(1).mean(1))\n",
    "    print(avg_performance.mean(1).mean(1))\n",
    "    torch.save([map_performance, avg_performance, gini_coefficients], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 1000\n",
    "num_episodes = 30\n",
    "sequence_length = 10\n",
    "\n",
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        ranking = True\n",
    "        direction = False\n",
    "\n",
    "        file_names = [\n",
    "            'trained_models/ranking_1_0.pth',\n",
    "            'trained_models/ranking_2_0.pth',\n",
    "            'trained_models/ranking_3_0.pth',\n",
    "            'trained_models/ranking_4_0.pth',\n",
    "            'trained_models/ranking_5_0.pth',\n",
    "            'trained_models/ranking_6_0.pth',\n",
    "            'trained_models/ranking_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "        save_path = 'data/bmli_ranking.pth'\n",
    "        num_features = 4\n",
    "\n",
    "    elif condition == 2:\n",
    "        ranking = False\n",
    "        direction = True\n",
    "\n",
    "        file_names = [\n",
    "            'trained_models/direction_1_0.pth',\n",
    "            'trained_models/direction_2_0.pth',\n",
    "            'trained_models/direction_3_0.pth',\n",
    "            'trained_models/direction_4_0.pth',\n",
    "            'trained_models/direction_5_0.pth',\n",
    "            'trained_models/direction_6_0.pth',\n",
    "            'trained_models/direction_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "        save_path = 'data/bmli_direction.pth'\n",
    "        num_features = 4\n",
    "\n",
    "    elif condition == 3:\n",
    "        ranking = False\n",
    "        direction = False\n",
    "\n",
    "        file_names = [\n",
    "            'trained_models/none_1_0.pth',\n",
    "            'trained_models/none_2_0.pth',\n",
    "            'trained_models/none_3_0.pth',\n",
    "            'trained_models/none_4_0.pth',\n",
    "            'trained_models/none_5_0.pth',\n",
    "            'trained_models/none_6_0.pth',\n",
    "            'trained_models/none_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "        save_path = 'data/bmli_none.pth'\n",
    "        num_features = 4\n",
    "        \n",
    "    elif condition == 4:\n",
    "        ranking = False\n",
    "        direction = False\n",
    "\n",
    "        file_names = [\n",
    "            'trained_models/none_2cues_1_full_0.pth',\n",
    "            'trained_models/none_2cues_2_full_0.pth',\n",
    "            'trained_models/none_2cues_3_full_0.pth',\n",
    "            'trained_models/none_2cues_4_full_0.pth',\n",
    "            'trained_models/none_2cues_5_full_0.pth',\n",
    "            'trained_models/none_2cues_6_full_0.pth',\n",
    "            'trained_models/none_2cues_pretrained_0.pth'\n",
    "        ]\n",
    "\n",
    "        save_path = 'data/bmli_none_2cues.pth'\n",
    "        num_features = 2\n",
    "        \n",
    "\n",
    "    data_loader = PairedComparison(num_features, ranking=ranking, direction=direction, dichotomized=False)\n",
    "\n",
    "    gini_coefficients = torch.zeros(len(file_names), num_experiments, num_episodes, sequence_length)\n",
    "    map_performance = torch.zeros(len(file_names), num_experiments, num_episodes, sequence_length)\n",
    "    avg_performance = torch.zeros(len(file_names), num_experiments, num_episodes, sequence_length)\n",
    "\n",
    "    for m, file_name in enumerate(file_names):\n",
    "        model = GRU(data_loader.num_inputs, data_loader.num_targets, 128)\n",
    "\n",
    "        params, _ = torch.load(file_name, map_location='cpu')\n",
    "        model.load_state_dict(params)\n",
    "\n",
    "        for k in tqdm(range(num_experiments)):\n",
    "            for i in range(num_episodes):\n",
    "                inputs, targets, _, _ = data_loader.get_batch(1, sequence_length)\n",
    "                predictive_distribution, weights, variances = model(inputs, targets)\n",
    "\n",
    "                map_performance[m, k, i] = ((predictive_distribution.probs > 0.5).float() == targets).squeeze().detach()\n",
    "                avg_performance[m, k, i] = ((1 - targets) * (1 - predictive_distribution.probs) + targets * predictive_distribution.probs).squeeze().detach()\n",
    "\n",
    "                for j in range(sequence_length):\n",
    "                    gini_coefficients[m, k, i, j] = gini(torch.abs(weights[j].t()).squeeze().detach().cpu().numpy())\n",
    "\n",
    "    print(map_performance.mean(1).mean(1))\n",
    "    torch.save([map_performance, avg_performance, gini_coefficients], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL Divergences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "num_episodes = 30\n",
    "sequence_length = 10\n",
    "\n",
    "for condition in [0, 1, 2]:\n",
    "    if condition == 0:\n",
    "        save_path = 'data/kl_ranking.pth'\n",
    "        ranking = True\n",
    "        direction = False\n",
    "        models = [VariationalEqualWeighting, VariationalFirstCue]\n",
    "        file_names = [\n",
    "            'trained_models/ranking_2_0.pth',\n",
    "            'trained_models/ranking_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "    elif condition == 1:\n",
    "        save_path = 'data/kl_direction.pth'\n",
    "        ranking = False\n",
    "        direction = True\n",
    "        models = [VariationalEqualWeighting, VariationalBestCue]\n",
    "        file_names = [\n",
    "            'trained_models/direction_2_0.pth',\n",
    "            'trained_models/direction_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "    elif condition == 2:\n",
    "        save_path = 'data/kl_none.pth'\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        models = [VariationalEqualWeighting, VariationalBestCue]\n",
    "        file_names = [\n",
    "            'trained_models/none_2_0.pth',\n",
    "            'trained_models/none_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "    data_loader = PairedComparison(4, ranking=ranking, direction=direction, dichotomized=False)\n",
    "    kl_divergences = torch.zeros(3, len(models), num_experiments, num_episodes, sequence_length)\n",
    "\n",
    "    bmi = GRU(4, 1, 128)\n",
    "    params_bmi, _ = torch.load(file_names[0], map_location='cpu')\n",
    "    bmi.load_state_dict(params_bmi)\n",
    "\n",
    "    mi = GRU(4, 1, 128)\n",
    "    params_mi, _ = torch.load(file_names[1], map_location='cpu')\n",
    "    mi.load_state_dict(params_mi)\n",
    "\n",
    "    for k in tqdm(range(num_experiments)):\n",
    "        for i in range(num_episodes):\n",
    "            inputs, targets, _, _ = data_loader.get_batch(1, sequence_length)\n",
    "\n",
    "            io = VariationalProbitRegression(data_loader.num_inputs, noise=data_loader.sigma)\n",
    "            io_pd = io.forward(inputs, targets)\n",
    "\n",
    "            bmi_pd, _, _ = bmi(inputs, targets)\n",
    "            mi_pd, _, _ = mi(inputs, targets)\n",
    "\n",
    "            for j, model_class in enumerate(models):\n",
    "                model = model_class(data_loader.num_inputs, noise=data_loader.sigma)\n",
    "                predictive_distribution = model.forward(inputs, targets)\n",
    "                kl_divergences[0, j, k, i] = kl_divergence(predictive_distribution, bmi_pd).squeeze()\n",
    "                kl_divergences[1, j, k, i] = kl_divergence(predictive_distribution, mi_pd).squeeze()\n",
    "                kl_divergences[2, j, k, i] = kl_divergence(predictive_distribution, io_pd).squeeze()\n",
    "\n",
    "\n",
    "    torch.save([kl_divergences], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 4:\n",
    "        load_path = \"data/humans_2features.pth\"\n",
    "        num_features = 2\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, _, _, _, _ = torch.load(load_path)\n",
    "        \n",
    "    save_path = 'data/logprobs' + str(condition) + '_fitted.pth'\n",
    "\n",
    "    if condition == 1:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalFirstCue]\n",
    "    else:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[0], len(models))\n",
    "    params = torch.zeros(inputs_a.shape[0], len(models))\n",
    "    for participant in tqdm(range(inputs_a.shape[0])):\n",
    "        for m, model_class in enumerate(models):\n",
    "            def f(sigma):\n",
    "                current_logprobs = 0\n",
    "                for task in range(inputs_a.shape[2]):\n",
    "                    model = model_class(num_features, noise=sigma[0, 0])\n",
    "                    participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                    participant_targets = targets[participant, :, [task]]\n",
    "                    predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                    current_logprobs = current_logprobs + predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "                return -current_logprobs\n",
    "\n",
    "            myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}])\n",
    "            myBopt.run_optimization(max_iter=100)\n",
    "            print(myBopt.x_opt)\n",
    "            print(myBopt.fx_opt)\n",
    "            params[participant, m] = myBopt.x_opt[0]\n",
    "            logprobs[participant, m] = -myBopt.fx_opt\n",
    "\n",
    "    print(logprobs / 2.303)\n",
    "    print(torch.argmax(logprobs, -1))\n",
    "\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Selection Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        ranking = True\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        ranking = False\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        ranking = False\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 4:\n",
    "        load_path = \"data/humans_2features.pth\"\n",
    "        ranking = False\n",
    "        num_features = 2\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, _, _, _, _ = torch.load(load_path)\n",
    "\n",
    "    save_path = 'data/logprobs' + str(condition) + '_selection_fitted.pth'\n",
    "\n",
    "    models = [StrategySelection]\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[0], len(models))\n",
    "    params = torch.zeros(inputs_a.shape[0], len(models))\n",
    "    for participant in tqdm(range(inputs_a.shape[0])):\n",
    "        for m, model_class in enumerate(models):\n",
    "            def f(sigma):\n",
    "                current_logprobs = 0\n",
    "                for task in range(inputs_a.shape[2]):\n",
    "                    model = model_class(num_features, ranking=ranking, noise=sigma[0, 0])\n",
    "                    participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                    participant_targets = targets[participant, :, [task]]\n",
    "                    predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                    current_logprobs = current_logprobs + predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "                return -current_logprobs\n",
    "\n",
    "            myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}])\n",
    "            myBopt.run_optimization(max_iter=100)\n",
    "            print(myBopt.x_opt)\n",
    "            print(myBopt.fx_opt)\n",
    "            params[participant, m] = myBopt.x_opt[0]\n",
    "            logprobs[participant, m] = -myBopt.fx_opt\n",
    "\n",
    "    print(logprobs / 2.303)\n",
    "    print(params)\n",
    "\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Network Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        model_class = FeedForward\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        model_class = FeedForward\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        model_class = FeedForward\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 4:\n",
    "        load_path = \"data/humans_2features.pth\"\n",
    "        model_class = FeedForward\n",
    "        num_features = 2\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, _, _, _, _ = torch.load(load_path)\n",
    "        \n",
    "\n",
    "    save_path = 'data/logprobs' + str(condition) + '_feedforward_fitted.pth'\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[0])\n",
    "    params = torch.zeros(inputs_a.shape[0], 2)\n",
    "    for participant in tqdm(range(inputs_a.shape[0])):\n",
    "        def f(params):\n",
    "            current_logprobs = 0\n",
    "            for task in range(inputs_a.shape[2]):\n",
    "                model = model_class(num_features, noise=params[0, 0], learning_rate=params[0, 1])\n",
    "                participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                participant_targets = targets[participant, :, [task]]\n",
    "                predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                current_logprobs = current_logprobs + predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "            return -current_logprobs\n",
    "\n",
    "        myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}, {'name': 'var_2', 'type': 'continuous', 'domain': (0.0, 0.1)}])\n",
    "        myBopt.run_optimization(max_iter=100)\n",
    "        print(torch.from_numpy(myBopt.x_opt))\n",
    "        print(myBopt.fx_opt)\n",
    "        logprobs[participant] = -myBopt.fx_opt\n",
    "        params[participant, :] = torch.from_numpy(myBopt.x_opt)\n",
    "\n",
    "    print(logprobs / 2.303)\n",
    "    print(params)\n",
    "\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BMI Model Fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "    elif condition == 4:\n",
    "        load_path = \"data/humans_2features.pth\"\n",
    "        num_features = 2\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, _, _, _, _ = torch.load(load_path)\n",
    "        \n",
    "    save_path = 'data/logprobs' + str(condition) + '_bmli.pth'\n",
    "\n",
    "    \n",
    "\n",
    "    if condition == 1:\n",
    "        model_paths = [\n",
    "            'trained_models/ranking_1_0.pth',\n",
    "            'trained_models/ranking_2_0.pth',\n",
    "            'trained_models/ranking_3_0.pth',\n",
    "            'trained_models/ranking_4_0.pth',\n",
    "            'trained_models/ranking_5_0.pth',\n",
    "            'trained_models/ranking_6_0.pth',\n",
    "            'trained_models/ranking_pretrained_0.pth'\n",
    "            ]\n",
    "    elif condition == 2:\n",
    "        model_paths = [\n",
    "            'trained_models/direction_1_0.pth',\n",
    "            'trained_models/direction_2_0.pth',\n",
    "            'trained_models/direction_3_0.pth',\n",
    "            'trained_models/direction_4_0.pth',\n",
    "            'trained_models/direction_5_0.pth',\n",
    "            'trained_models/direction_6_0.pth',\n",
    "            'trained_models/direction_pretrained_0.pth'\n",
    "            ]\n",
    "\n",
    "    elif condition == 3:\n",
    "        model_paths = [\n",
    "            'trained_models/none_1_0.pth',\n",
    "            'trained_models/none_2_0.pth',\n",
    "            'trained_models/none_3_0.pth',\n",
    "            'trained_models/none_4_0.pth',\n",
    "            'trained_models/none_5_0.pth',\n",
    "            'trained_models/none_6_0.pth',\n",
    "            'trained_models/none_pretrained_0.pth'\n",
    "            ]\n",
    "    elif condition == 4:\n",
    "        model_paths = [\n",
    "            'trained_models/none_2cues_1_full_0.pth',\n",
    "            'trained_models/none_2cues_2_full_0.pth',\n",
    "            'trained_models/none_2cues_3_full_0.pth',\n",
    "            'trained_models/none_2cues_4_full_0.pth',\n",
    "            'trained_models/none_2cues_5_full_0.pth',\n",
    "            'trained_models/none_2cues_6_full_0.pth',\n",
    "            'trained_models/none_2cues_pretrained_0.pth'\n",
    "        ]\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[0], len(model_paths))\n",
    "    # for each participant\n",
    "    with torch.no_grad():\n",
    "        for participant in tqdm(range(inputs_a.shape[0])):\n",
    "            # for each model\n",
    "            for m, model_path in enumerate(model_paths):\n",
    "                model = GRU(num_features, 1, 128)\n",
    "\n",
    "                params, _ = torch.load(model_path, map_location='cpu')\n",
    "                model.load_state_dict(params)\n",
    "\n",
    "                participant_inputs = inputs_a[participant] - inputs_b[participant]\n",
    "                participant_targets = targets[participant]\n",
    "\n",
    "                avg_probs = 0\n",
    "                for sample in range(num_samples):\n",
    "                    predictive_distribution, _, _ = model(participant_inputs, participant_targets)\n",
    "                    avg_probs += predictive_distribution.probs\n",
    "\n",
    "                avg_predictive_distribution = Bernoulli(avg_probs / num_samples)\n",
    "                logprobs[participant, m] = avg_predictive_distribution.log_prob(predictions[participant]).sum()\n",
    "\n",
    "        print(logprobs / 2.303)\n",
    "        print(torch.argmax(logprobs, -1))\n",
    "        torch.save([logprobs], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3, 4]:\n",
    "    load_path = 'data/logprobs' + str(condition) + '_fitted.pth'\n",
    "    logprobs = torch.load(load_path)[0]\n",
    "    logprobs[:, 1:] = -0.5 * math.log(300) + logprobs[:, 1:]\n",
    "\n",
    "    # add strategy selection\n",
    "    load_path = 'data/logprobs' + str(condition) + '_selection_fitted.pth'\n",
    "    logprobs_selection = torch.load(load_path)[0]\n",
    "    logprobs_selection = -0.5 * math.log(300) + logprobs_selection\n",
    "    logprobs = torch.cat([logprobs, logprobs_selection], dim=-1)\n",
    "\n",
    "    # add feedforward\n",
    "    logprobs_feedforward = torch.load('data/logprobs' + str(condition) + '_feedforward_fitted.pth')[0]\n",
    "    logprobs_feedforward = -0.5 * 2 * math.log(300) + logprobs_feedforward\n",
    "    logprobs = torch.cat([logprobs, logprobs_feedforward.unsqueeze(1)], dim=-1)\n",
    "\n",
    "    if condition >= 3:\n",
    "        logprobs_meta = torch.load('data/logprobs' + str(condition) + '_bmli.pth')[0]\n",
    "        logprobs_meta = torch.cat([logprobs[:, [0]], logprobs_meta], dim=-1)\n",
    "        best_logprobs, best_index = torch.max(logprobs_meta, dim=-1)\n",
    "        bic = -0.5 * 1 * math.log(300) + best_logprobs\n",
    "        logprobs = torch.cat([logprobs, bic.unsqueeze(1)], dim=-1)\n",
    "    torch.save(logprobs.detach(), 'data/evidence' + str(condition) + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-likelihoods for each time-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 100\n",
    "for condition in [1, 2, 3, 4]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        _, baseline_parameters = torch.load('data/logprobs1_fitted.pth')\n",
    "        bmi_logprobs = torch.load('data/logprobs' + str(condition) + '_bmli.pth')\n",
    "        model_paths = [\n",
    "            'trained_models/ranking_1_0.pth',\n",
    "            'trained_models/ranking_2_0.pth',\n",
    "            'trained_models/ranking_3_0.pth',\n",
    "            'trained_models/ranking_4_0.pth',\n",
    "            'trained_models/ranking_5_0.pth',\n",
    "            'trained_models/ranking_6_0.pth',\n",
    "            'trained_models/ranking_pretrained_0.pth'\n",
    "            ]\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        _, baseline_parameters = torch.load('data/logprobs2_fitted.pth')\n",
    "        bmi_logprobs = torch.load('data/logprobs' + str(condition) + '_bmli.pth')\n",
    "        model_paths = [\n",
    "            'trained_models/direction_1_0.pth',\n",
    "            'trained_models/direction_2_0.pth',\n",
    "            'trained_models/direction_3_0.pth',\n",
    "            'trained_models/direction_4_0.pth',\n",
    "            'trained_models/direction_5_0.pth',\n",
    "            'trained_models/direction_6_0.pth',\n",
    "            'trained_models/direction_pretrained_0.pth'\n",
    "            ]\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        _, baseline_parameters = torch.load('data/logprobs3_fitted.pth')\n",
    "        bmi_logprobs = torch.load('data/logprobs' + str(condition) + '_bmli.pth')\n",
    "        model_paths = [\n",
    "            'trained_models/none_1_0.pth',\n",
    "            'trained_models/none_2_0.pth',\n",
    "            'trained_models/none_3_0.pth',\n",
    "            'trained_models/none_4_0.pth',\n",
    "            'trained_models/none_5_0.pth',\n",
    "            'trained_models/none_6_0.pth',\n",
    "            'trained_models/none_pretrained_0.pth'\n",
    "            ]\n",
    "        num_features = 4\n",
    "        inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "        \n",
    "    elif condition == 4:\n",
    "        load_path = \"data/humans_2features.pth\"\n",
    "        _, baseline_parameters = torch.load('data/logprobs4_fitted.pth')\n",
    "        bmi_logprobs = torch.load('data/logprobs' + str(condition) + '_bmli.pth')\n",
    "        model_paths = [\n",
    "            'trained_models/none_2cues_1_full_0.pth',\n",
    "            'trained_models/none_2cues_2_full_0.pth',\n",
    "            'trained_models/none_2cues_3_full_0.pth',\n",
    "            'trained_models/none_2cues_4_full_0.pth',\n",
    "            'trained_models/none_2cues_5_full_0.pth',\n",
    "            'trained_models/none_2cues_6_full_0.pth',\n",
    "            'trained_models/none_2cues_pretrained_0.pth'\n",
    "        ]\n",
    "        num_features = 2\n",
    "        inputs_a, inputs_b, targets, predictions, time_elapsed, _, _, _, _ = torch.load(load_path)\n",
    "\n",
    "    bmi_index = torch.argmax(bmi_logprobs[0], -1)\n",
    "    save_path = 'data/logprobs' + str(condition) + '_time.pth'\n",
    "\n",
    "    if condition == 1:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalFirstCue]\n",
    "    else:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "\n",
    "    logprobs = torch.zeros(len(models) + 1, inputs_a.shape[0], inputs_a.shape[1], inputs_a.shape[2])\n",
    "    for m, model_class in enumerate(models):\n",
    "        for participant in tqdm(range(inputs_a.shape[0])):\n",
    "            for task in range(inputs_a.shape[2]):\n",
    "                model = model_class(num_features, noise=baseline_parameters[participant, m].item())\n",
    "                participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                participant_targets = targets[participant, :, [task]]\n",
    "                predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                logprobs[m, participant, :, task] = -predictive_distribution.log_prob(predictions[participant, :, [task]]).squeeze()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for participant in tqdm(range(inputs_a.shape[0])):\n",
    "            model = GRU(4, 1, 128)\n",
    "\n",
    "            params, _ = torch.load(model_paths[bmi_index[participant]], map_location='cpu')\n",
    "            model.load_state_dict(params)\n",
    "\n",
    "            participant_inputs = inputs_a[participant] - inputs_b[participant]\n",
    "            participant_targets = targets[participant]\n",
    "\n",
    "            avg_probs = 0\n",
    "            for sample in range(num_samples):\n",
    "                predictive_distribution, _, _ = model(participant_inputs, participant_targets)\n",
    "                avg_probs += predictive_distribution.probs\n",
    "\n",
    "            avg_predictive_distribution = Bernoulli(avg_probs / num_samples)\n",
    "            logprobs[-1, participant] = -avg_predictive_distribution.log_prob(predictions[participant]).squeeze()\n",
    "\n",
    "    print(logprobs / 2.303) #\n",
    "\n",
    "    torch.save([logprobs], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judgements about Ranking and Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_a, inputs_b, targets, predictions, time_elapsed, weights, direction1, direction2, ranking = torch.load('data/humans_2features.pth')\n",
    "\n",
    "# ranking\n",
    "participant_ranking = ranking.squeeze() # 1 means attribut 0 is more important\n",
    "actual_ranking = (weights[:,0,:,0] > weights[:,0,:,1]).float()\n",
    "print((participant_ranking == actual_ranking).float().mean())\n",
    "\n",
    "participant_direction1 = (direction1.squeeze()).float() # 1 means positve\n",
    "actual_direction1 = (weights[:,0,:,0] > 0).float()\n",
    "print((participant_direction1 == actual_direction1).float().mean())\n",
    "\n",
    "participant_direction2 = (direction2.squeeze()).float() # 1 means positve\n",
    "actual_direction2 = (weights[:,0,:,1] > 0).float()\n",
    "print((participant_direction2 == actual_direction2).float().mean())\n",
    "\n",
    "_, baseline_parameters = torch.load('data/logprobs4_fitted.pth')\n",
    "baseline_parameters = baseline_parameters[:, 1]\n",
    "print(baseline_parameters.shape)\n",
    "\n",
    "io_ranking = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], 1)\n",
    "io_direction1 = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], 1)\n",
    "io_direction2 = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], 1)\n",
    "io_mean = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], 2)\n",
    "io_std = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], 2)\n",
    "for participant in tqdm(range(inputs_a.shape[0])):\n",
    "    for task in range(inputs_a.shape[2]):\n",
    "        model = VariationalProbitRegression(2, noise=baseline_parameters[participant].item())\n",
    "        participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "        participant_targets = targets[participant, :, [task]]\n",
    "        predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "        mean = model.weights[:, :, 0]\n",
    "        std = torch.exp(model.log_sigmas[:, :, 0])\n",
    "\n",
    "        io_mean[participant, task] = mean[-1, :]\n",
    "        io_std[participant, task] = std[-1, :]\n",
    "        mean_difference = mean[-1, 0] - mean[-1, 1]\n",
    "        std_difference = (std[-1, 0].pow(2) + std[-1, 1].pow(2)).sqrt()\n",
    "        io_ranking[participant, task] = Normal(torch.zeros([]), torch.ones([])).cdf(mean_difference / std_difference)\n",
    "        io_direction1[participant, task] = Normal(torch.zeros([]), torch.ones([])).cdf(mean[-1, 0] / std[-1, 0])\n",
    "        io_direction2[participant, task] = Normal(torch.zeros([]), torch.ones([])).cdf(mean[-1, 1] / std[-1, 1])\n",
    "\n",
    "\n",
    "model_paths = [\n",
    "    'trained_models/none_2cues_1_full_0.pth',\n",
    "    'trained_models/none_2cues_2_full_0.pth',\n",
    "    'trained_models/none_2cues_3_full_0.pth',\n",
    "    'trained_models/none_2cues_4_full_0.pth',\n",
    "    'trained_models/none_2cues_5_full_0.pth',\n",
    "    'trained_models/none_2cues_6_full_0.pth',\n",
    "    'trained_models/none_2cues_pretrained_0.pth'\n",
    "]\n",
    "\n",
    "num_samples = 100\n",
    "logprobs = torch.load('data/logprobs4_bmli.pth')[0]\n",
    "best_model = torch.argmax(logprobs, -1)\n",
    "\n",
    "bmi_ranking = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], num_samples)\n",
    "bmi_direction1 = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], num_samples)\n",
    "bmi_direction2 = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], num_samples)\n",
    "bmi_mean = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], num_samples, 2)\n",
    "bmi_std = torch.zeros(inputs_a.shape[0], inputs_a.shape[2], num_samples, 2)\n",
    "# for each participant\n",
    "with torch.no_grad():\n",
    "    for participant in range(inputs_a.shape[0]):\n",
    "        # for each model\n",
    "        print(model_paths[best_model[participant].item()])\n",
    "        model_path = model_paths[best_model[participant].item()]\n",
    "        model = GRU(2, 1, 128)\n",
    "\n",
    "        params, _ = torch.load(model_path, map_location='cpu')\n",
    "        model.load_state_dict(params)\n",
    "\n",
    "        participant_inputs = inputs_a[participant] - inputs_b[participant]\n",
    "        participant_targets = targets[participant]\n",
    "\n",
    "        for sample in range(num_samples):\n",
    "            predictive_distribution, mean, std = model(participant_inputs, participant_targets)\n",
    "\n",
    "            bmi_mean[participant, :, sample, :] = mean[-1, :]\n",
    "            bmi_std[participant, :, sample, :] = std[-1, :]\n",
    "            mean_difference = mean[-1, :, 0] - mean[-1, :, 1]\n",
    "            std_difference = (std[-1, :, 0].pow(2) + std[-1, :, 1].pow(2)).sqrt()\n",
    "            bmi_ranking[participant, :, sample] = Normal(torch.zeros([]), torch.ones([])).cdf(mean_difference / std_difference) #(mean[-1, :, 0] > mean[-1, :, 1]).float()\n",
    "            bmi_direction1[participant, :, sample] = Normal(torch.zeros([]), torch.ones([])).cdf(mean[-1, :, 0] / std[-1, :, 0])  #(mean[-1, :, 0] > 0).float() #\n",
    "            bmi_direction2[participant, :, sample] = Normal(torch.zeros([]), torch.ones([])).cdf(mean[-1, :, 1] / std[-1, :, 1]) #(mean[-1, :, 1] > 0).float()\n",
    "            print(mean.shape)\n",
    "\n",
    "torch.save([actual_ranking, actual_direction1, actual_direction2,\n",
    "    participant_ranking, participant_direction1, participant_direction2,\n",
    "    io_ranking, io_direction1, io_direction2, io_mean, io_std,\n",
    "    bmi_ranking, bmi_direction1, bmi_direction2, bmi_mean, bmi_std],\n",
    "    'data/judgements.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fits for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "\n",
    "    save_path = 'data/logprobs' + str(condition) + '_tasks_fitted.pth'\n",
    "\n",
    "    inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "\n",
    "    if condition == 1:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalFirstCue]\n",
    "    else:\n",
    "        models = [Guessing, VariationalProbitRegression, VariationalEqualWeighting, VariationalBestCue]\n",
    "\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[2], inputs_a.shape[0], len(models))\n",
    "    params = torch.zeros(inputs_a.shape[2], inputs_a.shape[0], len(models))\n",
    "    for task in tqdm(range(inputs_a.shape[2])):\n",
    "        for m, model_class in enumerate(models):\n",
    "            for participant in range(inputs_a.shape[0]):\n",
    "                def f(sigma):\n",
    "                    model = model_class(4, noise=sigma[0, 0])\n",
    "                    participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                    participant_targets = targets[participant, :, [task]]\n",
    "                    predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                    return -predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "                myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}])\n",
    "                myBopt.run_optimization(max_iter=100)\n",
    "                params[task, participant, m] = myBopt.x_opt[0]\n",
    "                logprobs[task, participant, m] = -myBopt.fx_opt\n",
    "            print(logprobs[task, :, m].sum())\n",
    "\n",
    "\n",
    "    print(logprobs / 2.303) #\n",
    "    print(torch.argmax(logprobs, -1))\n",
    "\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        ranking = True\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        ranking = False\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        ranking = False\n",
    "\n",
    "    save_path = 'data/logprobs' + str(condition) + '_tasks_selection_fitted.pth'\n",
    "\n",
    "    inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "\n",
    "    models = [StrategySelection]\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[2], inputs_a.shape[0], len(models))\n",
    "    params = torch.zeros(inputs_a.shape[2], inputs_a.shape[0], len(models))\n",
    "    for task in tqdm(range(inputs_a.shape[2])):\n",
    "        for m, model_class in enumerate(models):\n",
    "            for participant in range(inputs_a.shape[0]):\n",
    "                def f(sigma):\n",
    "                    model = model_class(4, noise=sigma[0, 0], ranking=ranking)\n",
    "                    participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                    participant_targets = targets[participant, :, [task]]\n",
    "                    predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                    return -predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "                myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}])\n",
    "                myBopt.run_optimization(max_iter=100)\n",
    "                params[task, participant, m] = myBopt.x_opt[0]\n",
    "                logprobs[task, participant, m] = -myBopt.fx_opt\n",
    "            print(logprobs[task, :, m].sum())\n",
    "\n",
    "\n",
    "    print(logprobs / 2.303) #\n",
    "    print(torch.argmax(logprobs, -1))\n",
    "\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for condition in [1, 2, 3]:\n",
    "    if condition == 1:\n",
    "        load_path = \"data/humans_ranking.pth\"\n",
    "        ranking = True\n",
    "    elif condition == 2:\n",
    "        load_path = \"data/humans_direction.pth\"\n",
    "        ranking = False\n",
    "    elif condition == 3:\n",
    "        load_path = \"data/humans_none.pth\"\n",
    "        ranking = False\n",
    "\n",
    "    save_path = 'data/logprobs' + str(condition) + '_tasks_feedforward_fitted.pth'\n",
    "\n",
    "    inputs_a, inputs_b, targets, predictions, _ = torch.load(load_path)\n",
    "\n",
    "    learning_rates = [0.0, 1/256, 1/128, 1/64, 1/32, 1/16]\n",
    "\n",
    "    logprobs = torch.zeros(inputs_a.shape[2], inputs_a.shape[0])\n",
    "    params = torch.zeros(inputs_a.shape[2], inputs_a.shape[0], 2)\n",
    "    for task in tqdm(range(inputs_a.shape[2])):\n",
    "        for participant in range(inputs_a.shape[0]):\n",
    "            def f(params):\n",
    "                model = FeedForward(4,  noise=params[0, 0], learning_rate=params[0, 1])\n",
    "                participant_inputs = inputs_a[participant, :, [task]] - inputs_b[participant, :, [task]]\n",
    "                participant_targets = targets[participant, :, [task]]\n",
    "                predictive_distribution = model.forward(participant_inputs, participant_targets)\n",
    "                return -predictive_distribution.log_prob(predictions[participant, :, [task]]).sum().item()\n",
    "\n",
    "            myBopt = BayesianOptimization(f=f, domain=[{'name': 'var_1', 'type': 'continuous', 'domain': (0.01, 10)}, {'name': 'var_2', 'type': 'continuous', 'domain': (0.0, 0.1)}])\n",
    "            myBopt.run_optimization(max_iter=100)\n",
    "            params[task, participant, :] = torch.from_numpy(myBopt.x_opt)\n",
    "            logprobs[task, participant] = -myBopt.fx_opt\n",
    "        print(logprobs[task, :].sum())\n",
    "\n",
    "    print(logprobs / 2.303) #\n",
    "    torch.save([logprobs, params], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "num_tasks = 10000\n",
    "\n",
    "kl = torch.zeros(2, num_tasks, num_steps)\n",
    "\n",
    "for k, dichotomized in enumerate([True, False]):\n",
    "    data_loader = PairedComparison(4, direction=False, dichotomized=dichotomized, ranking=True)\n",
    "    for i in tqdm(range(num_tasks)):\n",
    "        if dichotomized:\n",
    "            model1 = VariationalFirstDiscriminatingCue(data_loader.num_inputs)\n",
    "        else:\n",
    "            model1 = VariationalFirstCue(data_loader.num_inputs)\n",
    "        model2 = VariationalProbitRegression(data_loader.num_inputs)\n",
    "        inputs, targets, _, _ = data_loader.get_batch(1, num_steps)\n",
    "\n",
    "        predictive_distribution1 = model1.forward(inputs, targets)\n",
    "        predictive_distribution2 = model2.forward(inputs, targets)\n",
    "        kl[k, i, :] =  kl_divergence(predictive_distribution1, predictive_distribution2).squeeze()\n",
    "\n",
    "torch.save(kl, 'data/power_analysis.pth')\n",
    "print(kl.mean(1))\n",
    "print(kl.mean(1).sum(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Selection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "num_episodes = 30\n",
    "sequence_length = 10\n",
    "\n",
    "for condition in [0, 1, 2, 3]:\n",
    "    if condition == 0:\n",
    "        save_path = 'data/additional_baselines_ranking.pth'\n",
    "        models = [StrategySelection] \n",
    "        ranking = True\n",
    "        direction = False\n",
    "        num_features = 4\n",
    "    elif condition == 1:\n",
    "        save_path = 'data/additional_baselines_direction.pth'\n",
    "        models = [StrategySelection] \n",
    "        ranking = False\n",
    "        direction = True\n",
    "        num_features = 4\n",
    "    elif condition == 2:\n",
    "        save_path = 'data/additional_baselines_none.pth'\n",
    "        models = [StrategySelection]\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 4\n",
    "    elif condition == 3:\n",
    "        save_path = 'data/additional_baselines_none_2features.pth'\n",
    "        models = [StrategySelection]\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 2\n",
    "\n",
    "    data_loader = PairedComparison(num_features, ranking=ranking, direction=direction, dichotomized=False)\n",
    "\n",
    "    map_performance = torch.zeros(len(models), num_experiments, num_episodes, sequence_length)\n",
    "    avg_performance = torch.zeros(len(models), num_experiments, num_episodes, sequence_length)\n",
    "    selected_models = torch.zeros(num_experiments, num_episodes, sequence_length, 3)\n",
    "\n",
    "    for j, model_class in enumerate(models):\n",
    "        for k in tqdm(range(num_experiments)):\n",
    "            for i in range(num_episodes):\n",
    "                inputs, targets, _, _ = data_loader.get_batch(1, sequence_length)\n",
    "                model = model_class(data_loader.num_inputs, noise=data_loader.sigma, ranking=ranking)\n",
    "                predictive_distribution = model.forward(inputs, targets)\n",
    "                selected_models[k, i] = model.selected_model\n",
    "\n",
    "                prediction = (predictive_distribution.probs > 0.5).float()\n",
    "                map_performance[j, k, i] = (prediction == targets).squeeze()\n",
    "                avg_performance[j, k, i] = ((1 - targets) * (1 - predictive_distribution.probs) + targets * predictive_distribution.probs).squeeze()\n",
    "\n",
    "    print(map_performance.mean(1).mean(1))\n",
    "    print(avg_performance.mean(1).mean(1))\n",
    "    torch.save([map_performance, avg_performance, selected_models], save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experiments = 100\n",
    "num_episodes = 30\n",
    "sequence_length = 10\n",
    "\n",
    "for condition in [0, 1, 2, 3]:\n",
    "    if condition == 0:\n",
    "        save_path = 'data/feedforward_baselines_ranking.pth'\n",
    "        ranking = True\n",
    "        direction = False\n",
    "        num_features = 4 \n",
    "    elif condition == 1:\n",
    "        save_path = 'data/feedforward_baselines_direction.pth'\n",
    "        ranking = False\n",
    "        direction = True\n",
    "        num_features = 4 \n",
    "    elif condition == 2:\n",
    "        save_path = 'data/feedforward_baselines_none.pth'\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 4 \n",
    "    elif condition == 3:\n",
    "        save_path = 'data/feedforward_baselines_none_2features.pth'\n",
    "        ranking = False\n",
    "        direction = False\n",
    "        num_features = 2\n",
    "\n",
    "    data_loader = PairedComparison(num_features, ranking=ranking, direction=direction, dichotomized=False)\n",
    "\n",
    "    learning_rates = [0.0, 1/256, 1/128, 1/64, 1/32, 1/16]\n",
    "    map_performance = torch.zeros(len(learning_rates), num_experiments, num_episodes, sequence_length)\n",
    "    avg_performance = torch.zeros(len(learning_rates), num_experiments, num_episodes, sequence_length)\n",
    "    gini_coefficients = torch.zeros(len(learning_rates), num_experiments, num_episodes, sequence_length)\n",
    "\n",
    "    for j, learning_rate in enumerate(learning_rates):\n",
    "        for k in tqdm(range(num_experiments)):\n",
    "            for i in range(num_episodes):\n",
    "                inputs, targets, _, _ = data_loader.get_batch(1, sequence_length)\n",
    "                model = FeedForward(data_loader.num_inputs, learning_rate=learning_rate)\n",
    "                predictive_distribution = model.forward(inputs, targets)\n",
    "                prediction = (predictive_distribution.probs > 0.5).float()\n",
    "                map_performance[j, k, i] = (prediction == targets).squeeze()\n",
    "                avg_performance[j, k, i] = ((1 - targets) * (1 - predictive_distribution.probs) + targets * predictive_distribution.probs).squeeze()\n",
    "\n",
    "                for t in range(sequence_length):\n",
    "                    gini_coefficients[j, k, i, t] = gini(torch.abs(model.means[t].t()).squeeze().detach().cpu().numpy())\n",
    "\n",
    "    print(map_performance.mean(1).mean(1))\n",
    "    print(avg_performance.mean(1).mean(1))\n",
    "    torch.save([map_performance, avg_performance, gini_coefficients], save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
